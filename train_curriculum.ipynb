{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéì Curriculum Learning Training - Fixed for 0% Accuracy Issue\n",
        "\n",
        "**CRITICAL FIX APPLIED:** \n",
        "- ‚úÖ Removed special tokens (`<step>`, `<answer>`, etc.) from training format\n",
        "- ‚úÖ Model now trains on simple \"Problem: X Solution: Y\" format\n",
        "- ‚úÖ This fixes the format mismatch that caused 0% accuracy\n",
        "\n",
        "**Root Cause:** The model was trained with complex special tokens but evaluated with simple prompts, causing complete mismatch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BxZRtC48kuK",
        "outputId": "cda5f8c0-f5d0-48b9-c8eb-c44b9411e9d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU Available: NVIDIA A100-SXM4-80GB\n",
            "   Memory: 85.17 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "    print(\"   Training on CPU will be 50x slower.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeuLh0mQ83y-",
        "outputId": "69a70671-7461-4483-dfbd-2f11a7d3d475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'AI-Mathematical-Olympiad'...\n",
            "remote: Enumerating objects: 236, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 236 (delta 49), reused 77 (delta 34), pack-reused 115 (from 1)\u001b[K\n",
            "Receiving objects: 100% (236/236), 11.43 MiB | 17.60 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n",
            "/content/AI-Mathematical-Olympiad\n",
            "‚úÖ Repository cloned and dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q datasets transformers tokenizers tqdm matplotlib\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/Alpyaman/AI-Mathematical-Olympiad.git\n",
        "%cd AI-Mathematical-Olympiad\n",
        "\n",
        "print(\"‚úÖ Repository cloned and dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cTRl5zqb86Nk"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "from src.config.model_config import MathTransformerConfig\n",
        "from src.model.decoder import MathTransformerDecoder\n",
        "from src.tokenizer.math_tokenizer import MathTokenizer\n",
        "from src.data.dataset import MathReasoningDataset, create_dataloaders, split_dataset\n",
        "from src.data.data_schema import MathProblem, MathSolution, ReasoningStep, DifficultyLevel, ProblemType\n",
        "from src.data.data_formatter import ChainOfThoughtFormatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0bW1V8wF9Byt"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    # Model - Medium model (~350M params) for better capacity\n",
        "    \"hidden_size\": 768,        # GPT-2 medium size\n",
        "    \"num_layers\": 12,          # Deeper network\n",
        "    \"num_heads\": 12,           # More attention heads\n",
        "    \"intermediate_size\": 3072, # 4x hidden size\n",
        "    \"max_length\": 512,         # Longer sequences for complex reasoning\n",
        "\n",
        "    # Training\n",
        "    \"batch_size\": 2,           # Small batch for large model\n",
        "    \"grad_accum\": 16,          # High accumulation to maintain effective batch of 32\n",
        "    \"learning_rate\": 2e-4,     # Lower LR for large model stability\n",
        "    \"epochs_per_stage\": 20,    # More epochs for thorough learning\n",
        "\n",
        "    # Curriculum stages\n",
        "    \"problems_per_stage\": 2000,\n",
        "\n",
        "    # Output\n",
        "    \"checkpoint_dir\": \"checkpoints/curriculum\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CehtdNwU9E0G"
      },
      "outputs": [],
      "source": [
        "def create_stage1_arithmetic(num_problems=2000, tokenizer=None):\n",
        "    \"\"\"Stage 1: Basic single-operation arithmetic\"\"\"\n",
        "    problems = []\n",
        "    random.seed(42)\n",
        "\n",
        "    operations = [\n",
        "        ('addition', lambda a, b: a + b, '+'),\n",
        "        ('subtraction', lambda a, b: a - b, '-'),\n",
        "        ('multiplication', lambda a, b: a * b, '√ó'),\n",
        "    ]\n",
        "\n",
        "    for i in range(num_problems):\n",
        "        op_name, op_func, op_symbol = random.choice(operations)\n",
        "\n",
        "        if op_name == 'addition':\n",
        "            a, b = random.randint(1, 50), random.randint(1, 50)\n",
        "        elif op_name == 'subtraction':\n",
        "            a, b = random.randint(20, 100), random.randint(1, 19)\n",
        "        else:  # multiplication\n",
        "            a, b = random.randint(2, 12), random.randint(2, 12)\n",
        "\n",
        "        answer = op_func(a, b)\n",
        "\n",
        "        problems.append(MathProblem(\n",
        "            problem_id=f\"s1_{i}\",\n",
        "            problem_statement=f\"What is {a} {op_symbol} {b}?\",\n",
        "            solution=MathSolution(\n",
        "                steps=[ReasoningStep(1, op_name.title(),\n",
        "                      f\"{a} {op_symbol} {b} = {answer}\", None)],\n",
        "                final_answer=str(answer),\n",
        "                answer_type=\"integer\"\n",
        "            ),\n",
        "            difficulty=DifficultyLevel.EASY,\n",
        "            problem_type=ProblemType.ALGEBRA,\n",
        "            topics=[\"arithmetic\"],\n",
        "            source=\"curriculum_stage1\"\n",
        "        ))\n",
        "\n",
        "    return problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN95kpOR9Hty"
      },
      "outputs": [],
      "source": [
        "def create_stage2_multistep(num_problems=2000, tokenizer=None):\n",
        "    \"\"\"Stage 2: Two-step arithmetic\"\"\"\n",
        "    problems = []\n",
        "    random.seed(43)\n",
        "\n",
        "    for i in range(num_problems):\n",
        "        # a + b - c or a * b + c\n",
        "        choice = random.choice(['add_sub', 'mul_add'])\n",
        "\n",
        "        if choice == 'add_sub':\n",
        "            a, b, c = random.randint(10, 30), random.randint(5, 20), random.randint(1, 10)\n",
        "            answer = a + b - c\n",
        "            problem_text = f\"Calculate {a} + {b} - {c}\"\n",
        "            steps_text = f\"First: {a} + {b} = {a+b}, then {a+b} - {c} = {answer}\"\n",
        "        else:  # mul_add\n",
        "            a, b, c = random.randint(2, 10), random.randint(2, 10), random.randint(1, 20)\n",
        "            answer = a * b + c\n",
        "            problem_text = f\"Calculate {a} √ó {b} + {c}\"\n",
        "            steps_text = f\"First: {a} √ó {b} = {a*b}, then {a*b} + {c} = {answer}\"\n",
        "\n",
        "        problems.append(MathProblem(\n",
        "            problem_id=f\"s2_{i}\",\n",
        "            problem_statement=problem_text,\n",
        "            solution=MathSolution(\n",
        "                steps=[ReasoningStep(1, \"Multistep\", steps_text, None)],\n",
        "                final_answer=str(answer),\n",
        "                answer_type=\"integer\"\n",
        "            ),\n",
        "            difficulty=DifficultyLevel.MEDIUM,\n",
        "            problem_type=ProblemType.ALGEBRA,\n",
        "            topics=[\"arithmetic\"],\n",
        "            source=\"curriculum_stage2\"\n",
        "        ))\n",
        "\n",
        "    return problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GFp4Qqu9KBT"
      },
      "outputs": [],
      "source": [
        "def create_stage3_algebra(num_problems=2000, tokenizer=None):\n",
        "    \"\"\"Stage 3: Simple linear equations\"\"\"\n",
        "    problems = []\n",
        "    random.seed(44)\n",
        "\n",
        "    for i in range(num_problems):\n",
        "        # ax + b = c, solve for x\n",
        "        a = random.randint(2, 10)\n",
        "        b = random.randint(1, 20)\n",
        "        x = random.randint(1, 20)\n",
        "        c = a * x + b\n",
        "\n",
        "        problems.append(MathProblem(\n",
        "            problem_id=f\"s3_{i}\",\n",
        "            problem_statement=f\"Solve for x: {a}x + {b} = {c}\",\n",
        "            solution=MathSolution(\n",
        "                steps=[\n",
        "                    ReasoningStep(1, \"Subtract\", f\"{a}x = {c} - {b} = {c-b}\", None),\n",
        "                    ReasoningStep(2, \"Divide\", f\"x = {c-b} / {a} = {x}\", None),\n",
        "                ],\n",
        "                final_answer=str(x),\n",
        "                answer_type=\"integer\"\n",
        "            ),\n",
        "            difficulty=DifficultyLevel.MEDIUM,\n",
        "            problem_type=ProblemType.ALGEBRA,\n",
        "            topics=[\"linear_equations\"],\n",
        "            source=\"curriculum_stage3\"\n",
        "        ))\n",
        "\n",
        "    return problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DiRbYBau9PUB"
      },
      "outputs": [],
      "source": [
        "def evaluate_quick(model, tokenizer, problems, device, num_samples=50):\n",
        "    \"\"\"Quick evaluation on sample problems\"\"\"\n",
        "    from src.evaluation.answer_extraction import AnswerExtractor, compare_answers\n",
        "\n",
        "    model.eval()\n",
        "    extractor = AnswerExtractor()\n",
        "\n",
        "    correct = 0\n",
        "    extracted = 0\n",
        "    sample_problems = random.sample(problems, min(num_samples, len(problems)))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for prob in sample_problems:\n",
        "            try:\n",
        "                # Use simple format matching training\n",
        "                prompt = f\"Problem: {prob.problem_statement}\\n\\nSolution:\"\n",
        "                encoded = tokenizer.encode(prompt, add_special_tokens=False, max_length=256, truncation=True)\n",
        "                input_ids = torch.tensor([encoded['input_ids']]).to(device)\n",
        "\n",
        "                outputs = model.generate(\n",
        "                    input_ids,\n",
        "                    max_new_tokens=100,\n",
        "                    temperature=0.0,\n",
        "                    eos_token_id=tokenizer.eos_token_id,\n",
        "                )\n",
        "\n",
        "                generated = tokenizer.decode(\n",
        "                    outputs[0][input_ids.shape[1]:].tolist(),\n",
        "                    skip_special_tokens=True\n",
        "                )\n",
        "\n",
        "                predicted = extractor.extract(generated)\n",
        "                ground_truth = prob.solution.final_answer\n",
        "\n",
        "                if predicted:\n",
        "                    extracted += 1\n",
        "                    if compare_answers(predicted, ground_truth, tolerance=1e-4):\n",
        "                        correct += 1\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    accuracy = (correct / num_samples) * 100 if num_samples > 0 else 0\n",
        "    extraction = (extracted / num_samples) * 100 if num_samples > 0 else 0\n",
        "\n",
        "    return accuracy, extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyQxIX7r9P98"
      },
      "outputs": [],
      "source": [
        "def train_stage(stage_name, problems, model, tokenizer, optimizer, scheduler, device, config):\n",
        "    \"\"\"Train on one curriculum stage\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìö STAGE: {stage_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Problems: {len(problems)}\")\n",
        "\n",
        "    # Split data\n",
        "    train_probs, val_probs, _ = split_dataset(problems, 0.8, 0.15, 0.05)\n",
        "    print(f\"Train: {len(train_probs)}, Val: {len(val_probs)}\")\n",
        "\n",
        "    # CRITICAL FIX: Create formatter WITHOUT special tokens - just simple Q&A format\n",
        "    # add_eos_token=True ensures EOS token is added at the END of the formatted text\n",
        "    simple_formatter = ChainOfThoughtFormatter(\n",
        "        use_special_tokens=False,  # No <step>, <answer> etc - this was causing 0% accuracy!\n",
        "        include_step_numbers=False,\n",
        "        include_justifications=False,\n",
        "        add_verification=False,\n",
        "        add_eos_token=True,  # Add EOS at the end of the formatted text\n",
        "    )\n",
        "\n",
        "    # Create datasets with simple formatter\n",
        "    train_ds = MathReasoningDataset(\n",
        "        train_probs, tokenizer,\n",
        "        formatter=simple_formatter,  # Use simple formatter\n",
        "        max_length=config[\"max_length\"]\n",
        "    )\n",
        "    val_ds = MathReasoningDataset(\n",
        "        val_probs, tokenizer,\n",
        "        formatter=simple_formatter,  # Use simple formatter\n",
        "        max_length=config[\"max_length\"]\n",
        "    )\n",
        "\n",
        "    train_loader, val_loader = create_dataloaders(\n",
        "        train_ds, val_ds,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(config[\"epochs_per_stage\"]):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs_per_stage']}\")\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for step, batch in enumerate(pbar):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs['loss'] / config[\"grad_accum\"]\n",
        "            loss.backward()\n",
        "\n",
        "            if (step + 1) % config[\"grad_accum\"] == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * config[\"grad_accum\"]\n",
        "            pbar.set_postfix({'loss': f\"{loss.item() * config['grad_accum']:.4f}\"})\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_steps = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                val_loss += outputs['loss'].item()\n",
        "                val_steps += 1\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / val_steps\n",
        "\n",
        "        # Quick accuracy check\n",
        "        accuracy, extraction = evaluate_quick(model, tokenizer, val_probs, device, num_samples=30)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.1f}%, \"\n",
        "              f\"Extraction: {extraction:.1f}%\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            print(f\"   ‚úÖ Best model for {stage_name}\")\n",
        "\n",
        "    return best_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fWTZ2qv9UM8"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéì CURRICULUM LEARNING - SIMPLE TO COMPLEX\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Device: {device}\\n\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = MathTokenizer()\n",
        "\n",
        "    # Create model\n",
        "    config = MathTransformerConfig(\n",
        "        vocab_size=len(tokenizer),\n",
        "        hidden_size=CONFIG[\"hidden_size\"],\n",
        "        num_hidden_layers=CONFIG[\"num_layers\"],\n",
        "        num_attention_heads=CONFIG[\"num_heads\"],\n",
        "        intermediate_size=CONFIG[\"intermediate_size\"],\n",
        "        max_position_embeddings=CONFIG[\"max_length\"],\n",
        "        max_sequence_length=CONFIG[\"max_length\"],\n",
        "    )\n",
        "\n",
        "    model = MathTransformerDecoder(config).to(device)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model: {total_params:,} parameters\\n\")\n",
        "\n",
        "    # Optimizer & Scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=0.01)\n",
        "    total_epochs = CONFIG[\"epochs_per_stage\"] * 3  # 3 stages\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs * 100, eta_min=1e-6)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = Path(CONFIG[\"checkpoint_dir\"])\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Generate curriculum data\n",
        "    print(\"üìö Generating curriculum data...\\n\")\n",
        "    stage1_problems = create_stage1_arithmetic(CONFIG[\"problems_per_stage\"], tokenizer=tokenizer)\n",
        "    stage2_problems = create_stage2_multistep(CONFIG[\"problems_per_stage\"], tokenizer=tokenizer)\n",
        "    stage3_problems = create_stage3_algebra(CONFIG[\"problems_per_stage\"], tokenizer=tokenizer)\n",
        "\n",
        "    print(f\"Stage 1 (Arithmetic): {len(stage1_problems)} problems\")\n",
        "    print(f\"Stage 2 (Multi-step): {len(stage2_problems)} problems\")\n",
        "    print(f\"Stage 3 (Algebra):    {len(stage3_problems)} problems\")\n",
        "\n",
        "    # Show example of how data will be formatted\n",
        "    print(\"\\nüìù Example training format (EOS token added at end):\")\n",
        "    print(f\"{'='*70}\")\n",
        "    simple_formatter = ChainOfThoughtFormatter(\n",
        "        use_special_tokens=False,\n",
        "        include_step_numbers=False,\n",
        "        include_justifications=False,\n",
        "        add_verification=False,\n",
        "        add_eos_token=True,  # EOS at the end\n",
        "    )\n",
        "    sample_text = simple_formatter.format_problem(stage1_problems[0], include_solution=True)\n",
        "    print(sample_text)\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"‚òùÔ∏è  EOS token is added at the END after 'Final Answer: X'\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Train each stage\n",
        "    stages = [\n",
        "        (\"Stage 1: Basic Arithmetic\", stage1_problems),\n",
        "        (\"Stage 2: Multi-step Arithmetic\", stage2_problems),\n",
        "        (\"Stage 3: Simple Algebra\", stage3_problems),\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for stage_name, problems in stages:\n",
        "        val_loss = train_stage(stage_name, problems, model, tokenizer,\n",
        "                              optimizer, scheduler, device, CONFIG)\n",
        "\n",
        "        # Save checkpoint after each stage\n",
        "        stage_num = stage_name.split()[1].rstrip(':')\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            'config': config,\n",
        "            'stage': stage_name,\n",
        "        }, checkpoint_dir / f\"stage{stage_num}_complete.pt\")\n",
        "\n",
        "        results[stage_name] = val_loss\n",
        "\n",
        "        print(f\"\\n‚úÖ {stage_name} complete! Val loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Final evaluation\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"üéØ FINAL EVALUATION\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Test on all stages\n",
        "    all_test = stage1_problems[-200:] + stage2_problems[-200:] + stage3_problems[-200:]\n",
        "    final_acc, final_ext = evaluate_quick(model, tokenizer, all_test, device, num_samples=100)\n",
        "\n",
        "    print(f\"Final Test Accuracy:  {final_acc:.1f}%\")\n",
        "    print(f\"Answer Extraction:    {final_ext:.1f}%\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': config,\n",
        "        'val_loss': val_loss,\n",
        "        'final_accuracy': final_acc,\n",
        "    }, checkpoint_dir / \"curriculum_final.pt\")\n",
        "\n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        'model': {\n",
        "            'parameters': total_params,\n",
        "            'config': {\n",
        "                'hidden_size': CONFIG[\"hidden_size\"],\n",
        "                'num_layers': CONFIG[\"num_layers\"],\n",
        "            }\n",
        "        },\n",
        "        'training': {\n",
        "            'problems_per_stage': CONFIG[\"problems_per_stage\"],\n",
        "            'epochs_per_stage': CONFIG[\"epochs_per_stage\"],\n",
        "            'stages': list(results.keys()),\n",
        "        },\n",
        "        'results': {\n",
        "            'final_accuracy': final_acc,\n",
        "            'final_extraction': final_ext,\n",
        "            'stage_losses': results,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(checkpoint_dir / \"curriculum_metadata.json\", 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"‚úÖ CURRICULUM TRAINING COMPLETE!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Final model saved to: {checkpoint_dir / 'curriculum_final.pt'}\")\n",
        "    print(f\"Expected accuracy on simple problems: {final_acc:.1f}%\")\n",
        "\n",
        "    if final_acc >= 20:\n",
        "        print(\"\\nüéâ SUCCESS! Model learned basic math reasoning!\")\n",
        "        print(\"   Ready to try more complex problems or fine-tuning.\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  Model needs more training or capacity.\")\n",
        "\n",
        "    print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ix1ykptz9ZK8",
        "outputId": "a0ad62a6-5f94-431b-ad3e-7ae03de53fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üéì CURRICULUM LEARNING - SIMPLE TO COMPLEX\n",
            "======================================================================\n",
            "\n",
            "Device: cuda\n",
            "\n",
            "Model: 114,100,992 parameters\n",
            "\n",
            "üìö Generating curriculum data...\n",
            "\n",
            "Stage 1 (Arithmetic): 2000 problems\n",
            "Stage 2 (Multi-step): 2000 problems\n",
            "Stage 3 (Algebra):    2000 problems\n",
            "\n",
            "üìù Example training format (simple, no special tokens):\n",
            "======================================================================\n",
            "Problem: What is 3 √ó 2?\n",
            "\n",
            "Solution:\n",
            "\n",
            "Multiplication\n",
            "  3 √ó 2 = 6\n",
            "\n",
            "\n",
            "Final Answer: 6\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üìö STAGE: Stage 1: Basic Arithmetic\n",
            "======================================================================\n",
            "Problems: 2000\n",
            "Train: 1600, Val: 300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:58<00:00, 13.72it/s, loss=0.4195]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: Train Loss: 1.5157, Val Loss: 0.3779, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:57<00:00, 13.97it/s, loss=0.2368]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2: Train Loss: 0.3185, Val Loss: 0.2951, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.13it/s, loss=0.1369]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3: Train Loss: 0.2693, Val Loss: 0.2259, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.18it/s, loss=0.1887]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4: Train Loss: 0.1701, Val Loss: 0.1449, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.15it/s, loss=0.1243]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5: Train Loss: 0.1406, Val Loss: 0.1414, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.18it/s, loss=0.1283]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6: Train Loss: 0.1339, Val Loss: 0.1338, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.17it/s, loss=0.1219]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7: Train Loss: 0.1301, Val Loss: 0.1329, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.14it/s, loss=0.1115]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8: Train Loss: 0.1279, Val Loss: 0.1302, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.20it/s, loss=0.1163]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9: Train Loss: 0.1249, Val Loss: 0.1282, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.20it/s, loss=0.1083]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10: Train Loss: 0.1208, Val Loss: 0.1251, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.19it/s, loss=0.1338]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11: Train Loss: 0.1186, Val Loss: 0.1230, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.19it/s, loss=0.1427]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12: Train Loss: 0.1164, Val Loss: 0.1756, Accuracy: 0.0%, Extraction: 100.0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.21it/s, loss=0.1013]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13: Train Loss: 0.1204, Val Loss: 0.1223, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.19it/s, loss=0.0795]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14: Train Loss: 0.1138, Val Loss: 0.1232, Accuracy: 0.0%, Extraction: 100.0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.19it/s, loss=0.0988]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15: Train Loss: 0.1109, Val Loss: 0.1190, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.19it/s, loss=0.1510]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16: Train Loss: 0.1069, Val Loss: 0.1223, Accuracy: 0.0%, Extraction: 100.0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.17it/s, loss=0.0780]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17: Train Loss: 0.1053, Val Loss: 0.1180, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.19it/s, loss=0.1124]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18: Train Loss: 0.1015, Val Loss: 0.1161, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.21it/s, loss=0.1345]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 19: Train Loss: 0.1016, Val Loss: 0.1143, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 1: Basic Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:56<00:00, 14.20it/s, loss=0.1038]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 20: Train Loss: 0.1033, Val Loss: 0.1195, Accuracy: 0.0%, Extraction: 100.0%\n",
            "\n",
            "‚úÖ Stage 1: Basic Arithmetic complete! Val loss: 0.1143\n",
            "\n",
            "======================================================================\n",
            "üìö STAGE: Stage 2: Multi-step Arithmetic\n",
            "======================================================================\n",
            "Problems: 2000\n",
            "Train: 1600, Val: 300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:57<00:00, 14.02it/s, loss=0.1266]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: Train Loss: 0.4571, Val Loss: 0.1244, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 2: Multi-step Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:58<00:00, 13.70it/s, loss=0.1180]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2: Train Loss: 0.1083, Val Loss: 0.1009, Accuracy: 0.0%, Extraction: 100.0%\n",
            "   ‚úÖ Best model for Stage 2: Multi-step Arithmetic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 516/800 [00:36<00:20, 14.00it/s, loss=0.0810]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3832242952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1579840990.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstage_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblems\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         val_loss = train_stage(stage_name, problems, model, tokenizer, \n\u001b[0m\u001b[1;32m     71\u001b[0m                               optimizer, scheduler, device, CONFIG)\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3729258842.py\u001b[0m in \u001b[0;36mtrain_stage\u001b[0;34m(stage_name, problems, model, tokenizer, optimizer, scheduler, device, config)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"grad_accum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"grad_accum\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
