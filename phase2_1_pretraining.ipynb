{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2.1: Base Pre-Training for Mathematical Reasoning\n",
    "\n",
    "This notebook demonstrates the complete pre-training infrastructure for training a decoder-only transformer on mixed mathematical and general text corpora.\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "**For GPU Training (Recommended):**\n",
    "1. Open this notebook in [Google Colab](https://colab.research.google.com)\n",
    "2. Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU (T4 or better)\n",
    "3. Run all cells\n",
    "\n",
    "**For CPU Testing (Local):**\n",
    "- Just run all cells (will use smaller model and fewer steps)\n",
    "\n",
    "## üì¶ What's Included\n",
    "\n",
    "- Streaming dataset for large-scale corpora\n",
    "- Mixed-domain sampling (ArXiv + General text)\n",
    "- Distributed training support (DDP)\n",
    "- Mixed precision (fp16/bf16)\n",
    "- Gradient accumulation\n",
    "- Learning rate scheduling\n",
    "- Automatic checkpointing\n",
    "- TensorBoard logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úì Running on Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚úì Running locally\")\n",
    "\n",
    "# Clone repository if on Colab\n",
    "if IN_COLAB:\n",
    "    print(\"\\nCloning repository...\")\n",
    "    !git clone https://github.com/Alpyaman/AI-Mathematical-Olympiad.git\n",
    "    %cd AI-Mathematical-Olympiad\n",
    "    !git checkout claude/setup-decoder-transformer-tpZw9\n",
    "    print(\"‚úì Repository cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install -q torch numpy tqdm\n",
    "\n",
    "# Optional: Install TensorBoard for logging\n",
    "!pip install -q tensorboard\n",
    "\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Phase 2.1 Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and tokenizer\n",
    "from src import (\n",
    "    get_small_config,\n",
    "    get_base_config,\n",
    "    MathTransformerDecoder,\n",
    "    MathTokenizer,\n",
    ")\n",
    "\n",
    "# Import training infrastructure\n",
    "from src.training import PreTrainer, PreTrainingConfig\n",
    "\n",
    "# Import data utilities\n",
    "from src.data.pretraining_dataset import (\n",
    "    create_sample_pretraining_data,\n",
    "    prepare_pretraining_data,\n",
    "    PreTrainingDataCollator,\n",
    ")\n",
    "\n",
    "print(\"‚úì All Phase 2.1 components imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Training\n",
    "\n",
    "We'll automatically adjust based on available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect hardware and configure accordingly\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "USE_GPU = device == \"cuda\"\n",
    "\n",
    "if USE_GPU:\n",
    "    # GPU Configuration - Faster training\n",
    "    print(\"üöÄ GPU Training Configuration\")\n",
    "    MODEL_SIZE = \"small\"  # Can use \"base\" for larger GPUs\n",
    "    BATCH_SIZE = 4\n",
    "    GRAD_ACCUM_STEPS = 8\n",
    "    MAX_STEPS = 500  # Increase to 10000+ for real training\n",
    "    MIXED_PRECISION = \"bf16\" if torch.cuda.is_bf16_supported() else \"fp16\"\n",
    "    NUM_WORKERS = 2\n",
    "else:\n",
    "    # CPU Configuration - Slower but still works\n",
    "    print(\"üíª CPU Training Configuration (Demo Mode)\")\n",
    "    MODEL_SIZE = \"small\"\n",
    "    BATCH_SIZE = 1\n",
    "    GRAD_ACCUM_STEPS = 2\n",
    "    MAX_STEPS = 20  # Very short for CPU demo\n",
    "    MIXED_PRECISION = \"fp32\"\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Model size: {MODEL_SIZE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "print(f\"  Max steps: {MAX_STEPS}\")\n",
    "print(f\"  Mixed precision: {MIXED_PRECISION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Pre-Training Data\n",
    "\n",
    "We'll create sample mathematical and general text data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "print(\"Creating sample pre-training data...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "data_dir = \"./data/pretraining_demo\"\n",
    "create_sample_pretraining_data(data_dir)\n",
    "\n",
    "print(\"\\n‚úì Sample data created!\")\n",
    "print(f\"  Location: {data_dir}\")\n",
    "print(f\"  ArXiv samples: 5 mathematical texts\")\n",
    "print(f\"  General samples: 5 general texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "print(\"Sample ArXiv text:\")\n",
    "print(\"=\" * 70)\n",
    "with open(f\"{data_dir}/arxiv/sample.jsonl\", 'r') as f:\n",
    "    sample = json.loads(f.readline())\n",
    "    print(sample['text'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Sample General text:\")\n",
    "print(\"=\" * 70)\n",
    "with open(f\"{data_dir}/general/sample.jsonl\", 'r') as f:\n",
    "    sample = json.loads(f.readline())\n",
    "    print(sample['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Tokenizer\n",
    "\n",
    "Our enhanced mathematical tokenizer with 200+ symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "print(\"Initializing mathematical tokenizer...\")\n",
    "tokenizer = MathTokenizer()\n",
    "\n",
    "print(f\"‚úì Tokenizer initialized\")\n",
    "print(f\"  Vocabulary size: {len(tokenizer):,}\")\n",
    "print(f\"  Special tokens: {tokenizer.special_tokens}\")\n",
    "print(f\"  Mathematical symbols: 200+\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"Let f: ‚Ñù ‚Üí ‚Ñù be continuous. Then ‚à´‚ÇÄ¬π f(x)dx exists.\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "\n",
    "print(f\"\\nTokenization test:\")\n",
    "print(f\"  Original: {test_text}\")\n",
    "print(f\"  Decoded:  {decoded}\")\n",
    "print(f\"  Tokens: {len(encoded['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Streaming Dataset\n",
    "\n",
    "Create a mixed-domain dataset that samples 30% from ArXiv and 70% from general text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare streaming dataset\n",
    "print(\"Preparing mixed-domain streaming dataset...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "train_dataset = prepare_pretraining_data(\n",
    "    data_dir=data_dir,\n",
    "    sources=[\"arxiv\", \"general\"],\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,  # Shorter for demo\n",
    "    mix_weights=[0.3, 0.7],  # 30% math, 70% general\n",
    ")\n",
    "\n",
    "print(\"‚úì Streaming dataset created\")\n",
    "print(f\"  Data sources: ArXiv (30%), General (70%)\")\n",
    "print(f\"  Streaming mode: Yes (memory efficient)\")\n",
    "print(f\"  Max sequence length: 512 tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "collator = PreTrainingDataCollator(pad_token_id=tokenizer.pad_token_id)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "print(\"‚úì Data loader created\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n",
    "\n",
    "# Test loading a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  Input shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"  Attention mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "print(f\"  Labels shape: {sample_batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Model\n",
    "\n",
    "Create the decoder-only transformer from Phase 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model configuration\n",
    "if MODEL_SIZE == \"small\":\n",
    "    config = get_small_config()\n",
    "    # Further reduce for demo if on CPU\n",
    "    if not USE_GPU:\n",
    "        config.hidden_size = 256\n",
    "        config.num_hidden_layers = 4\n",
    "        config.num_attention_heads = 4\n",
    "        config.num_key_value_heads = 4\n",
    "        config.intermediate_size = 1024\n",
    "elif MODEL_SIZE == \"base\":\n",
    "    config = get_base_config()\n",
    "\n",
    "# Update vocab size to match tokenizer\n",
    "config.vocab_size = len(tokenizer)\n",
    "config.max_position_embeddings = 512\n",
    "\n",
    "# Initialize model\n",
    "print(f\"Initializing {MODEL_SIZE} model...\")\n",
    "model = MathTransformerDecoder(config)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úì Model initialized\")\n",
    "print(f\"  Architecture: Decoder-only (Llama-style)\")\n",
    "print(f\"  Hidden size: {config.hidden_size}\")\n",
    "print(f\"  Layers: {config.num_hidden_layers}\")\n",
    "print(f\"  Attention heads: {config.num_attention_heads}\")\n",
    "print(f\"  Parameters: {num_params:,} ({num_trainable:,} trainable)\")\n",
    "print(f\"  Positional encoding: RoPE (dynamic scaling)\")\n",
    "print(f\"  Activation: SwiGLU\")\n",
    "\n",
    "# Show model size in MB\n",
    "param_size_mb = num_params * 4 / (1024 ** 2)  # 4 bytes per float32 param\n",
    "print(f\"  Model size: {param_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configure Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training configuration\n",
    "training_config = PreTrainingConfig(\n",
    "    model_config_name=MODEL_SIZE,\n",
    "    vocab_size=config.vocab_size,\n",
    "    max_seq_length=512,\n",
    "    data_dir=data_dir,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    micro_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    warmup_steps=min(50, MAX_STEPS // 10),\n",
    "    learning_rate=3e-4,\n",
    "    \n",
    "    # Optimization\n",
    "    mixed_precision=MIXED_PRECISION,\n",
    "    gradient_checkpointing=USE_GPU,  # Only on GPU\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint_dir=\"./checkpoints/pretraining_notebook\",\n",
    "    save_interval=max(100, MAX_STEPS // 2),\n",
    "    \n",
    "    # Logging\n",
    "    log_interval=5 if not USE_GPU else 10,\n",
    "    use_wandb=False,\n",
    "    use_tensorboard=False,  # Disabled for cleaner output\n",
    "    \n",
    "    # System\n",
    "    num_workers=NUM_WORKERS,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"‚úì Training configuration created\")\n",
    "print(f\"\\nConfiguration summary:\")\n",
    "print(f\"  Effective batch size: {training_config.effective_batch_size}\")\n",
    "print(f\"  Total steps: {training_config.max_steps}\")\n",
    "print(f\"  Warmup steps: {training_config.warmup_steps}\")\n",
    "print(f\"  Peak learning rate: {training_config.learning_rate}\")\n",
    "print(f\"  Mixed precision: {training_config.mixed_precision}\")\n",
    "print(f\"  Gradient checkpointing: {training_config.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize Pre-Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pre-trainer\n",
    "print(\"Initializing Pre-Trainer...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "trainer = PreTrainer(\n",
    "    model=model,\n",
    "    config=training_config,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=None,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Pre-Trainer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Pre-Training\n",
    "\n",
    "This is where the magic happens! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING BASE PRE-TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if not USE_GPU:\n",
    "    print(\"‚ö†Ô∏è  Running on CPU - this will be slow!\")\n",
    "    print(\"    For faster training, use Google Colab with GPU\")\n",
    "    print()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ PRE-TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test the Trained Model\n",
    "\n",
    "Let's generate some text to see what the model learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=0.8, device=\"cuda\"):\n",
    "    \"\"\"Generate text continuation from a prompt.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    encoded = tokenizer.encode(prompt)\n",
    "    input_ids = torch.tensor([encoded['input_ids']], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    for _ in range(max_length):\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # Get next token logits\n",
    "        next_token_logits = outputs[0, -1, :] / temperature\n",
    "        \n",
    "        # Sample next token\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append to sequence\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "        \n",
    "        # Stop if EOS token\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Decode\n",
    "    generated_ids = input_ids[0].tolist()\n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "print(\"Testing text generation...\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Mathematical prompt\n",
    "math_prompt = \"Let f: ‚Ñù ‚Üí ‚Ñù be a continuous function. Then\"\n",
    "generated = generate_text(\n",
    "    model=trainer.raw_model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=math_prompt,\n",
    "    max_length=30,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Mathematical generation:\")\n",
    "print(f\"Prompt: {math_prompt}\")\n",
    "print(f\"Generated: {generated}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Theorem prompt\n",
    "theorem_prompt = \"Theorem: For any prime number p, we have\"\n",
    "generated = generate_text(\n",
    "    model=trainer.raw_model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=theorem_prompt,\n",
    "    max_length=30,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Theorem generation:\")\n",
    "print(f\"Prompt: {theorem_prompt}\")\n",
    "print(f\"Generated: {generated}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: General text prompt\n",
    "general_prompt = \"The history of mathematics began in\"\n",
    "generated = generate_text(\n",
    "    model=trainer.raw_model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=general_prompt,\n",
    "    max_length=30,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"General text generation:\")\n",
    "print(f\"Prompt: {general_prompt}\")\n",
    "print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save and Load Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint\n",
    "checkpoint_path = Path(training_config.checkpoint_dir) / \"final_notebook.pt\"\n",
    "trainer.save_checkpoint(\"final_notebook.pt\")\n",
    "\n",
    "print(f\"‚úì Checkpoint saved to: {checkpoint_path}\")\n",
    "print(f\"\\nCheckpoint contains:\")\n",
    "print(f\"  - Model weights\")\n",
    "print(f\"  - Optimizer state\")\n",
    "print(f\"  - Training step: {trainer.global_step}\")\n",
    "print(f\"  - Tokens seen: {trainer.tokens_seen:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load checkpoint\n",
    "if checkpoint_path.exists():\n",
    "    print(\"Loading checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    print(f\"\\n‚úì Checkpoint loaded\")\n",
    "    print(f\"  Training step: {checkpoint['global_step']}\")\n",
    "    print(f\"  Tokens seen: {checkpoint['tokens_seen']:,}\")\n",
    "    print(f\"  Config: {checkpoint['config']['model_config_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2.1 DEMONSTRATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úì Successfully demonstrated:\")\n",
    "print(\"  1. Streaming dataset for large-scale corpora\")\n",
    "print(\"  2. Mixed-domain data sampling (ArXiv + General)\")\n",
    "print(\"  3. Decoder-only transformer architecture\")\n",
    "print(\"  4. Pre-training with causal language modeling\")\n",
    "print(\"  5. Mixed precision training\" if USE_GPU else \"  5. CPU training (demo mode)\")\n",
    "print(\"  6. Automatic checkpointing\")\n",
    "print(\"  7. Text generation from trained model\")\n",
    "print()\n",
    "print(\"Training Statistics:\")\n",
    "print(f\"  Steps completed: {trainer.global_step}\")\n",
    "print(f\"  Tokens processed: {trainer.tokens_seen:,}\")\n",
    "print(f\"  Model parameters: {num_params:,}\")\n",
    "print()\n",
    "print(\"Next steps for FULL pre-training:\")\n",
    "print(\"  1. Prepare large-scale datasets:\")\n",
    "print(\"     - ArXiv papers (LaTeX extraction): ~2M papers\")\n",
    "print(\"     - C4 corpus: 750GB of web text\")\n",
    "print(\"     - Wikipedia: ~6M articles\")\n",
    "print(\"     - Books corpus\")\n",
    "print()\n",
    "print(\"  2. Scale up training:\")\n",
    "print(\"     - Use base or large model\")\n",
    "print(\"     - Train for 100K-1M steps\")\n",
    "print(\"     - Use multiple GPUs with DDP:\")\n",
    "print(\"       torchrun --nproc_per_node=4 pretrain.py\")\n",
    "print()\n",
    "print(\"  3. Monitor with wandb:\")\n",
    "print(\"     python pretrain.py --use-wandb --wandb-project my-project\")\n",
    "print()\n",
    "print(\"  4. Proceed to Phase 2.2: Mathematical Fine-tuning\")\n",
    "print(\"     - Fine-tune on MATH dataset\")\n",
    "print(\"     - Add reinforcement learning\")\n",
    "print(\"     - Outcome supervision\")\n",
    "print()\n",
    "print(\"Checkpoints saved to:\", training_config.checkpoint_dir)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. (Optional) Download Checkpoint\n",
    "\n",
    "If running on Colab, you can download the checkpoint to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Zip checkpoints\n",
    "    !zip -r checkpoints.zip checkpoints/\n",
    "    \n",
    "    print(\"Downloading checkpoint...\")\n",
    "    files.download('checkpoints.zip')\n",
    "    print(\"‚úì Download started\")\n",
    "else:\n",
    "    print(\"Checkpoints are saved locally at:\", training_config.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "**Documentation:**\n",
    "- See `PHASE_2_1_README.md` for comprehensive documentation\n",
    "- Run `python pretrain.py --help` for CLI options\n",
    "\n",
    "**Scaling Up:**\n",
    "```bash\n",
    "# Multi-GPU training (4 GPUs)\n",
    "torchrun --nproc_per_node=4 pretrain.py \\\n",
    "    --model-size base \\\n",
    "    --batch-size 4 \\\n",
    "    --gradient-accumulation-steps 8 \\\n",
    "    --max-steps 500000 \\\n",
    "    --mixed-precision bf16 \\\n",
    "    --use-wandb\n",
    "```\n",
    "\n",
    "**Key Papers:**\n",
    "- [Chinchilla: Training Compute-Optimal LLMs](https://arxiv.org/abs/2203.15556)\n",
    "- [LLaMA: Open Foundation LLMs](https://arxiv.org/abs/2302.13971)\n",
    "- [Minerva: Mathematical Reasoning](https://arxiv.org/abs/2206.14858)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Pre-Training! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
